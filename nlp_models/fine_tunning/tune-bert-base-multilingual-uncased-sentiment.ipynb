{"cells":[{"cell_type":"markdown","metadata":{"id":"h9Ps51pPEVTI"},"source":["# Packages and functions"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49523,"status":"ok","timestamp":1660054465364,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"TQ6WzAttEaSt","outputId":"70ef4571-534e-45cb-ef2b-69cbb40a2af9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","\n","[notice] A new release of pip available: 22.1.2 -> 22.2.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Could not find a version that satisfies the requirement ray[tune] (from versions: none)\n","ERROR: No matching distribution found for ray[tune]\n"]}],"source":["%pip install \"ray[tune]\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23579,"status":"ok","timestamp":1660054539240,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"uxz9bfmtEfgF","outputId":"808919e9-3912-4087-ac1f-c12ecf5204ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Installed kernelspec python3 in C:\\Users\\joseph\\AppData\\Roaming\\jupyter\\kernels\\python3\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DSUAWO3vEVTP"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from datasets import (load_metric, Dataset, DatasetDict)\n","from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline)\n","import torch\n","torch.manual_seed(0)\n","import ray.tune as tune\n","from ray.tune.suggest.hyperopt import HyperOptSearch\n","from ray.tune.schedulers import ASHAScheduler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %pip install torch==1.10.2+cu102 torchvision==0.11.3+cu102 torchaudio==0.10.2+cu102 --extra-index-url https://download.pytorch.org/whl/cu102"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nluzcvq4Er_2"},"outputs":[],"source":["# import os\n","# os.chdir('/content/drive/MyDrive/Colab Notebooks')"]},{"cell_type":"markdown","metadata":{"id":"wx2PDxQmOl39"},"source":["# Model 0 geolocation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43490,"status":"ok","timestamp":1660053792382,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"wjol_aOHPMPi","outputId":"b26d009d-c3d8-480e-b628-d2b0e03c8743"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 189700 testing size: 81300\n","Columns: Index(['Id', 'text', 'date', 'country', 'country_train'], dtype='object')\n"]}],"source":["model_path = 'model0_geolocation'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99icfUbpPjvF"},"outputs":[],"source":["n_epochs = 1 #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","text_column_name = \"text\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"country_train\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠"]},{"cell_type":"markdown","metadata":{"id":"ZiAQu3-gOm_I"},"source":["# Model 1 valid"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3935,"status":"ok","timestamp":1660054551438,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"K0N4TWMCZVB3","outputId":"62c1ca4c-00b8-4c0a-fb75-85ff40472587"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 2170 testing size: 930\n","Columns: Index(['Id', 'date', 'text_es', 'valid', 'tone', 'referred_to',\n","       'text_es_clean'],\n","      dtype='object')\n"]}],"source":["model_path = 'model1_valid'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6G_vU2J5ZVB8"},"outputs":[],"source":["text_column_name = \"text_es_clean\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"valid\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","train['valid'] = train['valid'].astype(str)\n","test['valid'] = test['valid'].astype(str)"]},{"cell_type":"markdown","metadata":{"id":"c4vPGFgfjrIY"},"source":["# Model 2 subject\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1659975927783,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"9g3ZMO6GRPEq","outputId":"b4a8fc04-29e0-47c7-835a-255ffb5f2494"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 824 testing size: 356\n","Columns: Index(['Id', 'date', 'text_es', 'valid', 'tone', 'referred_to',\n","       'text_es_clean'],\n","      dtype='object')\n"]}],"source":["model_path = 'model2_subject'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgO4U_GhRPEq"},"outputs":[],"source":["n_epochs = 20 #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","text_column_name = \"text_es_clean\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"referred_to\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠"]},{"cell_type":"markdown","metadata":{"id":"7Z-abtHGOvFb"},"source":["# Model 3 tone"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2941,"status":"ok","timestamp":1659981135710,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"kgSFYm9sldHv","outputId":"33989126-8daa-4daf-f3a4-4d1c9b66bcd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 1344 testing size: 576\n","Columns: Index(['Id', 'date', 'text_es', 'valid', 'tone', 'referred_to',\n","       'text_es_clean', 'text_tone'],\n","      dtype='object')\n"]}],"source":["model_path = 'model3_tone'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkxsQ1cVldHv"},"outputs":[],"source":["n_epochs = 10 #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","text_column_name = \"text_es_clean\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"text_tone\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠"]},{"cell_type":"markdown","metadata":{"id":"Pas60w6jO58F"},"source":["# Model 4 negativeness"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3161,"status":"ok","timestamp":1659984013876,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"QKbpixtqr41C","outputId":"3df0eff0-9daf-47ee-bfa2-1c9659fc06c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 441 testing size: 189\n","Columns: Index(['Id', 'date', 'text_es', 'valid', 'tone', 'referred_to',\n","       'text_es_clean'],\n","      dtype='object')\n"]}],"source":["model_path = 'model4_negativeness'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGVj7LkMr41H"},"outputs":[],"source":["n_epochs = 20 #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","text_column_name = \"text_es_clean\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"tone\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","train[cat_column_name] = train[cat_column_name].astype(str)\n","test[cat_column_name] = test[cat_column_name].astype(str)"]},{"cell_type":"markdown","metadata":{"id":"ykCoTWsEO8rI"},"source":["# Model 5 positiveness"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2137,"status":"ok","timestamp":1659985228692,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"O8gnahf_v1eD","outputId":"25322532-0618-49e1-e451-16909a84eb58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training size: 90 testing size: 42\n","Columns: Index(['Id', 'date', 'text_es', 'valid', 'tone', 'referred_to',\n","       'text_es_clean', 'pos_tone'],\n","      dtype='object')\n"]}],"source":["model_path = 'model5_positiveness'\n","train = pd.read_excel(model_path+'/train.xlsx')\n","test = pd.read_excel(model_path+'/test.xlsx')\n","print(\"Training size:\",len(train),\"testing size:\", len(test)) \n","print(\"Columns:\", train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LNMC7O_v1eI"},"outputs":[],"source":["n_epochs = 30 #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","text_column_name = \"text_es_clean\"  #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","cat_column_name = \"pos_tone\" #⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠⚠\n","train[cat_column_name] = train[cat_column_name].astype(str)\n","test[cat_column_name] = test[cat_column_name].astype(str)"]},{"cell_type":"markdown","metadata":{"id":"w4Ak0oRzPDIt"},"source":["# Running"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"L7db3QLnGm6Y"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"X337sZtxkQLG"},"outputs":[],"source":["out_dir = model_path+'/tune_bert_multi' \n","model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DgVCcEyHEVTR"},"outputs":[],"source":["cats = train[cat_column_name].unique()\n","n_labels = len(cats)\n","label2id = {}\n","id2label = {}\n","for i in range(len(cats)):\n","  label2id[cats[i]] = i\n","  id2label[i] = cats[i]\n","\n","train = train.rename(columns={text_column_name: 'text', cat_column_name: 'label'})\n","test = test.rename(columns={text_column_name: 'text', cat_column_name: 'label'})\n","train.reset_index(drop=True, inplace=True)\n","test.reset_index(drop=True, inplace=True)\n","train['label'] = train['label'].map(label2id)\n","test['label'] = test['label'].map(label2id)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"pZjdZhN2EVTT"},"outputs":[],"source":["train = train[['text', 'label']]\n","test = test[['text', 'label']]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"WeSbKo59m0QR"},"outputs":[],"source":["train['text'] = train['text'].astype(str)\n","test['text'] = test['text'].astype(str)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"a_wuGQAVEVTT"},"outputs":[],"source":["train_dataset = Dataset.from_dict(train)\n","test_dataset = Dataset.from_dict(test)\n","my_dataset_dict = DatasetDict({\"train\":train_dataset,\"test\":test_dataset})"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["c10ec025b2f44ef4a25dc90586161a18","4c42ae44729f47adb35265372b9dcb93","2f59a072fb5e4528b053a2fc6868c102","0a13f3d00c66481f8a27981d7d1ec203","563d8a43f3b8472bb026b32c70dee83e","a080d5016e9c4023aea33c77898cc477","71339afa7176422b80482f222912659c","19e376da5c964c0ea704c834c0513344","fb1e675ff2ab4dc2b7dbee3fb221f27c","e403a0164b3f45419b985c0f370e76fd","86da0e668dd94d06b3ce5cdfddcd0924","2dc9446d26f64eeb9d951633f3db4de8","a68b15c996a04fa69daef61f061b1da1","6a14340267364f558f10d97395833132","3706bf747839491cadbbcb760072aa68","36e33aa5c57a4fcfaf18c0809f91871e","4648b6bd388f49de8d562d45ba636831","00a63006b9444c8ebfb5a4a811f875c4","9747da49a3634c7f94c2b2b10f5610ca","4830c3b865ed4562a384a8a90a148e4a","6386237ccd4541ec9562a98f7e4a1f60","db4ed348cfeb400399521b88e37a29e9","04459566b3704f31903ed71c8bc67390","6b9e0aa50a9f49aa81f5e133f660aee8","8d2963cf20df4d3780bede782213f454","be33e259f6244528929b22d73eb1209e","d7fe48170da64f27b74e3a3c759103a8","bd0494ad57c44e42b378becf10e82bf0","aa8cee84e5854efc9946cdd616403b3a","13feae2c4d3a40ef9a952c12ccfce569","015d6c0dbae84ce2b004a90a6e923316","50c7c8173ee342a2ae186151d827aea8","9ac2f3140c854f4c80239fa50bacd54e","1d8b9a9206bf4b47b8764d61769dcb4a","48e1a88d829c416ea7c732b9a6ca04ec","7b1e25759aa94ebaba96de5d0b659571","186b196229f6426b80f7dc8198ebed73","384095b6abfa4fa9a31a04a0d5223895","fc6659052d604f66a1e0418a6b07d956","0aa9e9b52e6344e181eb704462528738","41c66ae6443547ac9463c4dd8fa163ce","bd4b5d044cc14e03a4c0886cffd23cc0","be9fed07ba074162b1d399e56045db40","b40cd8991a9d4562aabd4191bf21dffe"]},"executionInfo":{"elapsed":15069,"status":"ok","timestamp":1660054585176,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"wDHrqBP2EVTU","outputId":"61bf1cd3-0def-40ad-c451-fc6efecb3a66"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, id2label = id2label, label2id = label2id)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["48b053c1277149a9800bee21213fade7","0e3e9d02eb5543bbada0b0b9af72193e","465e8e5a536041d09c19056a9f046635","fc3b9a5fbef446c292f48fda61cda2ff","809bd70a6c994149b1979c04b1bbb9bc","6b6884fd8b614fbd909a89a709aeb929","665b522a32424532b99614cbfe1a7e9c","aa084c68c78e4b61a82698a1272dfcfc","b81d7ea4444d49699126a5c8528873ca","710872506ef946dfaf5eebee7d29a2e8","01e986eac15348a0b19bf46f56f92a1e","80bdfc8a8cf04180860ad7fed37fa549","bbcceaa27cd8469fa02b2baf3c2926fa","c129e28836ce46b59b62ea67c12a8910","643b8975ae8c48ebbc3a65c829dadf51","c98db12b71674895b5ed027b9f5ee064","4613b5dc898c4dc3b89ee3bf18f2d6a5","9dc81cc2d5084f798cad1439c1690b4d","795c06e39b5743269c0cd63e46b4f77b","36e3381be3ce4a4e90d901b71728451b","aaf2119139df4263a614c39cdfd14c03","24f10ec45d3743baa89999f7f61af12a"]},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1660054585679,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"_7VxVo9mEVTU","outputId":"4188c0aa-67c3-4884-ef9f-36900fe0303b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [00:00<00:00,  4.07ba/s]\n","100%|██████████| 1/1 [00:00<00:00,  3.56ba/s]\n"]}],"source":["def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = my_dataset_dict.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uF-rPUXQEVTV"},"outputs":[],"source":["small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=0) \n","small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=0) "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["246de9378df84d82a3731a33b2cd206c","801bebd0a4be49d389a101893b0af4d6","ee6af9dc1f514ef2a04e260f666d4660","79d72fd61ed848b3bafc4a9efaa80f71","954d1fd4a8604c8981bca8e05344eed4","5f6ee6102ca44ad49956ccb9cdb34bd2","a0d627c6801c4dbe9495136fb89cc996","67a6baa6d83f402cbe639a4427cca7a9","fdae8ec1203c48aeacb00ae0925208dc","1fe073a43ade48adb1e8aedaf40bf595","3c67bac3f7804f3c9fb25c2cb329d70a"]},"executionInfo":{"elapsed":16437,"status":"ok","timestamp":1660054602112,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"b2kvKo4HEVTW","outputId":"f1807875-e99f-4b46-db82-3a02f8a1b89b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label = id2label, label2id = label2id,\n","                                                           num_labels=n_labels, ignore_mismatched_sizes=True) \n","training_args = TrainingArguments(output_dir=out_dir)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"e8YlM6sNEVTW"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    metric1 = load_metric(\"accuracy\")\n","    accuracy = metric1.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    if n_labels == 2:\n","      metric2 = load_metric(\"precision\")\n","      metric3 = load_metric(\"recall\")\n","      metric4 = load_metric(\"f1\")\n","      precision = metric2.compute(predictions=predictions, references=labels, average = 'macro')[\"precision\"]\n","      recall = metric3.compute(predictions=predictions, references=labels, average = 'macro')[\"recall\"]\n","      f1 = metric4.compute(predictions=predictions, references=labels, average = 'macro')[\"f1\"]\n","      return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\":f1}\n","    elif n_labels > 2:\n","      return {\"accuracy\":accuracy}"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"dHmK-3XMEVTX"},"outputs":[],"source":["training_args = TrainingArguments(output_dir=out_dir, evaluation_strategy=\"epoch\",\n","                                  save_strategy = \"steps\", save_total_limit = 2, logging_strategy = \"epoch\",\n","                                  seed = 0, disable_tqdm=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"fbAaKy2fyObm"},"outputs":[],"source":["def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(model_name, id2label = id2label, label2id = label2id,\n","                                                           num_labels=n_labels, ignore_mismatched_sizes=True, return_dict=True)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8056,"status":"ok","timestamp":1660054610157,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"FyolUmacEVTX","outputId":"b307993c-eabb-46a0-a6f5-fae4dcc73eb2"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:346: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n","  warnings.warn(\n"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_eval_dataset,\n","    compute_metrics=compute_metrics,\n","    model_init = model_init\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10813,"status":"ok","timestamp":1660054620958,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"m6xHrIVqEVTY","outputId":"1c31b356-87f0-4c00-860a-4c44f01d893d"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"]}],"source":["for batch in trainer.get_train_dataloader():\n","    break\n","\n","outputs = trainer.model.cpu()(**batch)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7cuSemzKEVTY"},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 3.33 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\joseph\\OneDrive - Universidad del Norte\\Scripts VMASC\\Academic Research Twitter Search (10M)\\nlp_models\\tune-bert-base-multilingual-uncased-sentiment.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m outputs \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(device)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1556\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1556\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1557\u001b[0m     input_ids,\n\u001b[0;32m   1558\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1559\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1560\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1561\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1562\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1563\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1564\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1565\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1566\u001b[0m )\n\u001b[0;32m   1568\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1570\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1011\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1019\u001b[0m     embedding_output,\n\u001b[0;32m   1020\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1021\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1022\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1023\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1025\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1031\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    600\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    491\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    494\u001b[0m         hidden_states,\n\u001b[0;32m    495\u001b[0m         attention_mask,\n\u001b[0;32m    496\u001b[0m         head_mask,\n\u001b[0;32m    497\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    498\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    500\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    502\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 423\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    424\u001b[0m         hidden_states,\n\u001b[0;32m    425\u001b[0m         attention_mask,\n\u001b[0;32m    426\u001b[0m         head_mask,\n\u001b[0;32m    427\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    428\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    429\u001b[0m         past_key_value,\n\u001b[0;32m    430\u001b[0m         output_attentions,\n\u001b[0;32m    431\u001b[0m     )\n\u001b[0;32m    432\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    433\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:327\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    324\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    326\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    330\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 3.33 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","batch = {k: v.to(device) for k, v in batch.items()}\n","\n","outputs = trainer.model.to(device)(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660054624405,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"wcoERAGuFqAR","outputId":"c4c19308-0a50-488b-b453-6dc07bc7f2be"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDVpMdw1EVTZ"},"outputs":[],"source":["loss = outputs.loss\n","loss.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":487,"status":"ok","timestamp":1660054624882,"user":{"displayName":"JOSEPH MARTINEZ SALCEDO","userId":"17505158837290186759"},"user_tz":300},"id":"onyZcUZaEVTZ","outputId":"5a41d340-aa24-4fda-a4e1-7a4730480436"},"outputs":[{"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\joseph\\OneDrive - Universidad del Norte\\Scripts VMASC\\Academic Research Twitter Search (10M)\\nlp_models\\tune-bert-base-multilingual-uncased-sentiment.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mcreate_optimizer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n","\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"]}],"source":["trainer.create_optimizer()\n","trainer.optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"eozt0zkDwlZS"},"source":["# Training"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/resolve/main/config.json from cache at C:\\Users\\joseph/.cache\\huggingface\\transformers\\d9226eeac7b8b96d83ebc327cdd670490866d8c999505c1f83b6ef206ccb1604.a34960b447312b0727cb670d710444fcb41a6156eddcba062a19b3fc05d95251\n","Model config BertConfig {\n","  \"_name_or_path\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n","  \"_num_labels\": 5,\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": \"sentiment-analysis\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 105879\n","}\n","\n","loading weights file https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/resolve/main/pytorch_model.bin from cache at C:\\Users\\joseph/.cache\\huggingface\\transformers\\c3020f16ae496cb8ba53cdb83e08cca88c008e5c4263884ecdc4a8a6000e8751.2da3f39deb1fb7ac0e8bd6c41b5ded28013a75c0d779d283a57e6b0fe34d4091\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 2170\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 816\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\joseph\\OneDrive - Universidad del Norte\\Scripts VMASC\\Academic Research Twitter Search (10M)\\nlp_models\\tune-bert-base-multilingual-uncased-sentiment.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_tune \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mrun(trainer\u001b[39m.\u001b[39;49mtrain(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     config\u001b[39m=\u001b[39m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mper_device_train_batch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_train_epochs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: tune\u001b[39m.\u001b[39mgrid_search([\u001b[39m0.1\u001b[39m, \u001b[39m0.01\u001b[39m]), \u001b[39m#, 0.001, 0.0001\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     }, resources_per_trial\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joseph/OneDrive%20-%20Universidad%20del%20Norte/Scripts%20VMASC/Academic%20Research%20Twitter%20Search%20%2810M%29/nlp_models/tune-bert-base-multilingual-uncased-sentiment.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_samples\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1738\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1743\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1744\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1745\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1746\u001b[0m ):\n\u001b[0;32m   1747\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:2488\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2486\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2488\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2490\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[1;32mc:\\Users\\joseph\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["best_tune = tune.run(trainer.train(),\n","    config={\n","        \"per_device_train_batch_size\": 2,\n","        \"num_train_epochs\": 1,\n","        \"learning_rate\": tune.grid_search([0.1, 0.01]), #, 0.001, 0.0001\n","    }, resources_per_trial={'gpu': 1,'cpu': 1},\n","    num_samples=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["config={\n","        \"per_device_train_batch_size\": 2,\n","        \"num_train_epochs\": 1,\n","        \"learning_rate\": tune.grid_search([0.001, 0.0001]),\n","    }\n","\n","best_trial = trainer.hyperparameter_search(\n","    direction=\"maximize\", resources_per_trial={'gpu': 1,'cpu': 1}, config=config,\n","    backend='ray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPk0E82_ZMCi"},"outputs":[],"source":["trainer.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3H5jKNTMOtJ2"},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkyB1-vyH8MD"},"outputs":[],"source":["trainer.save_metrics('all',trainer.evaluate())"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["wx2PDxQmOl39","c4vPGFgfjrIY","7Z-abtHGOvFb","Pas60w6jO58F","ykCoTWsEO8rI","w4Ak0oRzPDIt"],"name":"tune-bert-base-multilingual-uncased-sentiment.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"954a6f92c48fca38f992f305027673bb9dc1ac69d50a77c523a6d9c197dc0ab3"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"00a63006b9444c8ebfb5a4a811f875c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"015d6c0dbae84ce2b004a90a6e923316":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01e986eac15348a0b19bf46f56f92a1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04459566b3704f31903ed71c8bc67390":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b9e0aa50a9f49aa81f5e133f660aee8","IPY_MODEL_8d2963cf20df4d3780bede782213f454","IPY_MODEL_be33e259f6244528929b22d73eb1209e"],"layout":"IPY_MODEL_d7fe48170da64f27b74e3a3c759103a8"}},"0a13f3d00c66481f8a27981d7d1ec203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e403a0164b3f45419b985c0f370e76fd","placeholder":"​","style":"IPY_MODEL_86da0e668dd94d06b3ce5cdfddcd0924","value":" 39.0/39.0 [00:00&lt;00:00, 1.01kB/s]"}},"0aa9e9b52e6344e181eb704462528738":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e3e9d02eb5543bbada0b0b9af72193e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b6884fd8b614fbd909a89a709aeb929","placeholder":"​","style":"IPY_MODEL_665b522a32424532b99614cbfe1a7e9c","value":"100%"}},"13feae2c4d3a40ef9a952c12ccfce569":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"186b196229f6426b80f7dc8198ebed73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be9fed07ba074162b1d399e56045db40","placeholder":"​","style":"IPY_MODEL_b40cd8991a9d4562aabd4191bf21dffe","value":" 112/112 [00:00&lt;00:00, 3.21kB/s]"}},"19e376da5c964c0ea704c834c0513344":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d8b9a9206bf4b47b8764d61769dcb4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48e1a88d829c416ea7c732b9a6ca04ec","IPY_MODEL_7b1e25759aa94ebaba96de5d0b659571","IPY_MODEL_186b196229f6426b80f7dc8198ebed73"],"layout":"IPY_MODEL_384095b6abfa4fa9a31a04a0d5223895"}},"1fe073a43ade48adb1e8aedaf40bf595":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246de9378df84d82a3731a33b2cd206c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_801bebd0a4be49d389a101893b0af4d6","IPY_MODEL_ee6af9dc1f514ef2a04e260f666d4660","IPY_MODEL_79d72fd61ed848b3bafc4a9efaa80f71"],"layout":"IPY_MODEL_954d1fd4a8604c8981bca8e05344eed4"}},"24f10ec45d3743baa89999f7f61af12a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dc9446d26f64eeb9d951633f3db4de8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a68b15c996a04fa69daef61f061b1da1","IPY_MODEL_6a14340267364f558f10d97395833132","IPY_MODEL_3706bf747839491cadbbcb760072aa68"],"layout":"IPY_MODEL_36e33aa5c57a4fcfaf18c0809f91871e"}},"2f59a072fb5e4528b053a2fc6868c102":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19e376da5c964c0ea704c834c0513344","max":39,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb1e675ff2ab4dc2b7dbee3fb221f27c","value":39}},"36e3381be3ce4a4e90d901b71728451b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36e33aa5c57a4fcfaf18c0809f91871e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3706bf747839491cadbbcb760072aa68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6386237ccd4541ec9562a98f7e4a1f60","placeholder":"​","style":"IPY_MODEL_db4ed348cfeb400399521b88e37a29e9","value":" 953/953 [00:00&lt;00:00, 28.1kB/s]"}},"384095b6abfa4fa9a31a04a0d5223895":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c67bac3f7804f3c9fb25c2cb329d70a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41c66ae6443547ac9463c4dd8fa163ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4613b5dc898c4dc3b89ee3bf18f2d6a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4648b6bd388f49de8d562d45ba636831":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"465e8e5a536041d09c19056a9f046635":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa084c68c78e4b61a82698a1272dfcfc","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b81d7ea4444d49699126a5c8528873ca","value":3}},"4830c3b865ed4562a384a8a90a148e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48b053c1277149a9800bee21213fade7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e3e9d02eb5543bbada0b0b9af72193e","IPY_MODEL_465e8e5a536041d09c19056a9f046635","IPY_MODEL_fc3b9a5fbef446c292f48fda61cda2ff"],"layout":"IPY_MODEL_809bd70a6c994149b1979c04b1bbb9bc"}},"48e1a88d829c416ea7c732b9a6ca04ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc6659052d604f66a1e0418a6b07d956","placeholder":"​","style":"IPY_MODEL_0aa9e9b52e6344e181eb704462528738","value":"Downloading special_tokens_map.json: 100%"}},"4c42ae44729f47adb35265372b9dcb93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a080d5016e9c4023aea33c77898cc477","placeholder":"​","style":"IPY_MODEL_71339afa7176422b80482f222912659c","value":"Downloading tokenizer_config.json: 100%"}},"50c7c8173ee342a2ae186151d827aea8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"563d8a43f3b8472bb026b32c70dee83e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6ee6102ca44ad49956ccb9cdb34bd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6386237ccd4541ec9562a98f7e4a1f60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"643b8975ae8c48ebbc3a65c829dadf51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaf2119139df4263a614c39cdfd14c03","placeholder":"​","style":"IPY_MODEL_24f10ec45d3743baa89999f7f61af12a","value":" 1/1 [00:00&lt;00:00,  3.48ba/s]"}},"665b522a32424532b99614cbfe1a7e9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67a6baa6d83f402cbe639a4427cca7a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a14340267364f558f10d97395833132":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9747da49a3634c7f94c2b2b10f5610ca","max":953,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4830c3b865ed4562a384a8a90a148e4a","value":953}},"6b6884fd8b614fbd909a89a709aeb929":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b9e0aa50a9f49aa81f5e133f660aee8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd0494ad57c44e42b378becf10e82bf0","placeholder":"​","style":"IPY_MODEL_aa8cee84e5854efc9946cdd616403b3a","value":"Downloading vocab.txt: 100%"}},"710872506ef946dfaf5eebee7d29a2e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71339afa7176422b80482f222912659c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795c06e39b5743269c0cd63e46b4f77b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d72fd61ed848b3bafc4a9efaa80f71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fe073a43ade48adb1e8aedaf40bf595","placeholder":"​","style":"IPY_MODEL_3c67bac3f7804f3c9fb25c2cb329d70a","value":" 638M/638M [00:12&lt;00:00, 50.0MB/s]"}},"7b1e25759aa94ebaba96de5d0b659571":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41c66ae6443547ac9463c4dd8fa163ce","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd4b5d044cc14e03a4c0886cffd23cc0","value":112}},"801bebd0a4be49d389a101893b0af4d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f6ee6102ca44ad49956ccb9cdb34bd2","placeholder":"​","style":"IPY_MODEL_a0d627c6801c4dbe9495136fb89cc996","value":"Downloading pytorch_model.bin: 100%"}},"809bd70a6c994149b1979c04b1bbb9bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80bdfc8a8cf04180860ad7fed37fa549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbcceaa27cd8469fa02b2baf3c2926fa","IPY_MODEL_c129e28836ce46b59b62ea67c12a8910","IPY_MODEL_643b8975ae8c48ebbc3a65c829dadf51"],"layout":"IPY_MODEL_c98db12b71674895b5ed027b9f5ee064"}},"86da0e668dd94d06b3ce5cdfddcd0924":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d2963cf20df4d3780bede782213f454":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13feae2c4d3a40ef9a952c12ccfce569","max":871891,"min":0,"orientation":"horizontal","style":"IPY_MODEL_015d6c0dbae84ce2b004a90a6e923316","value":871891}},"954d1fd4a8604c8981bca8e05344eed4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9747da49a3634c7f94c2b2b10f5610ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ac2f3140c854f4c80239fa50bacd54e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dc81cc2d5084f798cad1439c1690b4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a080d5016e9c4023aea33c77898cc477":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0d627c6801c4dbe9495136fb89cc996":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a68b15c996a04fa69daef61f061b1da1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4648b6bd388f49de8d562d45ba636831","placeholder":"​","style":"IPY_MODEL_00a63006b9444c8ebfb5a4a811f875c4","value":"Downloading config.json: 100%"}},"aa084c68c78e4b61a82698a1272dfcfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa8cee84e5854efc9946cdd616403b3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaf2119139df4263a614c39cdfd14c03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b40cd8991a9d4562aabd4191bf21dffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b81d7ea4444d49699126a5c8528873ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbcceaa27cd8469fa02b2baf3c2926fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4613b5dc898c4dc3b89ee3bf18f2d6a5","placeholder":"​","style":"IPY_MODEL_9dc81cc2d5084f798cad1439c1690b4d","value":"100%"}},"bd0494ad57c44e42b378becf10e82bf0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd4b5d044cc14e03a4c0886cffd23cc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be33e259f6244528929b22d73eb1209e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50c7c8173ee342a2ae186151d827aea8","placeholder":"​","style":"IPY_MODEL_9ac2f3140c854f4c80239fa50bacd54e","value":" 851k/851k [00:01&lt;00:00, 583kB/s]"}},"be9fed07ba074162b1d399e56045db40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10ec025b2f44ef4a25dc90586161a18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c42ae44729f47adb35265372b9dcb93","IPY_MODEL_2f59a072fb5e4528b053a2fc6868c102","IPY_MODEL_0a13f3d00c66481f8a27981d7d1ec203"],"layout":"IPY_MODEL_563d8a43f3b8472bb026b32c70dee83e"}},"c129e28836ce46b59b62ea67c12a8910":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_795c06e39b5743269c0cd63e46b4f77b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36e3381be3ce4a4e90d901b71728451b","value":1}},"c98db12b71674895b5ed027b9f5ee064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7fe48170da64f27b74e3a3c759103a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db4ed348cfeb400399521b88e37a29e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e403a0164b3f45419b985c0f370e76fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee6af9dc1f514ef2a04e260f666d4660":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67a6baa6d83f402cbe639a4427cca7a9","max":669491321,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdae8ec1203c48aeacb00ae0925208dc","value":669491321}},"fb1e675ff2ab4dc2b7dbee3fb221f27c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc3b9a5fbef446c292f48fda61cda2ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_710872506ef946dfaf5eebee7d29a2e8","placeholder":"​","style":"IPY_MODEL_01e986eac15348a0b19bf46f56f92a1e","value":" 3/3 [00:00&lt;00:00,  3.23ba/s]"}},"fc6659052d604f66a1e0418a6b07d956":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdae8ec1203c48aeacb00ae0925208dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
